import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import Huber
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# 1) Load & preprocess
df = pd.read_excel(
    "buildings_10000_enriched.xlsx",
    na_values=["NA","unspecified","Unspecified"]
)
df = df.loc[:, ~df.columns.str.lower().str.contains("unnamed")]
df = df.dropna(
    subset=[c for c in df.columns if c not in [
        'country','no_floors','occupation','urban_rural',
        'avrg_income','avrg_housing_price'
    ]],
    how='all'
).fillna(0)

feature_cols = [
    'country','no_floors','occupation','urban_rural',
    'avrg_income','avrg_housing_price'
]
target_cols  = [c for c in df.columns if c not in feature_cols]

X = pd.get_dummies(df[feature_cols], drop_first=True).values.astype(np.float32)
y = df[target_cols].astype(float).values

scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)
scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y)

X_train, X_tmp, y_train, y_tmp = train_test_split(
    X_scaled, y_scaled, test_size=0.30, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_tmp, y_tmp, test_size=0.50, random_state=42
)

# 2) Best hyperparameters we found
best_hps = {
    "units_0":      128,
    "l2_0":   1.9974525520146183e-06,
    "dropout_0":      0.0,
    "learning_rate": 0.00032409501212600874
}

# 3) Build the model with a second hidden layer
model = Sequential([
    # first hidden block
    Dense(
        best_hps["units_0"],
        activation='relu',
        kernel_regularizer=l2(best_hps["l2_0"]),
        input_shape=(X_train.shape[1],)
    ),
    Dropout(best_hps["dropout_0"]),

    # second hidden block (new)
    Dense(
        64,              # try 64 units
        activation='relu',
        kernel_regularizer=l2(1e-6)  # small L2; tune as needed
    ),
    Dropout(0.10),      # 10% dropout; tune as needed

    # output layer
    Dense(y_train.shape[1], name='output')
])

# 4) Compile with Huber loss
model.compile(
    optimizer=Adam(learning_rate=best_hps["learning_rate"], clipnorm=1.0),
    loss=Huber(delta=1.0),
    metrics=['mae']
)

# 5) EarlyStopping
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=25,
    restore_best_weights=True,
    verbose=1
)

# 6) Train
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=500,
    batch_size=32,
    callbacks=[early_stop],
    verbose=2
)

# 7) Evaluate
test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)
print(f"Huber Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}")

# 8) Plot training curves
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(history.history['loss'],    label='Train Huber')
plt.plot(history.history['val_loss'],label='Val Huber')
plt.title('Huber Loss Over Epochs')
plt.xlabel('Epoch'); plt.ylabel('Loss')
plt.legend(); plt.grid(True)

plt.subplot(1,2,2)
plt.plot(history.history['mae'],     label='Train MAE')
plt.plot(history.history['val_mae'], label='Val MAE')
plt.title('MAE Over Epochs')
plt.xlabel('Epoch'); plt.ylabel('MAE')
plt.legend(); plt.grid(True)

plt.tight_layout()
plt.show()
